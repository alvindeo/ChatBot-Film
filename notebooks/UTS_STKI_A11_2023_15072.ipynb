{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "694297d2"
      },
      "source": [
        "## Tujuan Proyek\n",
        "\n",
        "Tujuan utama dari proyek ini adalah untuk membangun sistem chatbot sederhana berbasis Retrieval-Augmented Generation (RAG) menggunakan teknik Information Retrieval (IR) klasik untuk menjawab pertanyaan pengguna seputar topik film. Secara spesifik, tujuan proyek meliputi:\n",
        "\n",
        "1.  **Implementasi Model Information Retrieval:** Mengembangkan dan mengintegrasikan model Boolean Information Retrieval dan Vector Space Model (VSM) dengan pembobotan TF-IDF untuk pencarian dokumen yang relevan.\n",
        "2.  **Pembangunan Basis Pengetahuan (Knowledge Base):** Membuat korpus dokumen (pertanyaan dan jawaban) dari file teks yang disediakan untuk dijadikan sumber informasi bagi chatbot.\n",
        "3.  **Preprocessing Data:** Menerapkan teknik preprocessing teks (case folding, filtering, tokenisasi, stop word removal, stemming) untuk mempersiapkan dokumen dan query.\n",
        "4.  **Fungsi Tanya Jawab (Question Answering):** Mengembangkan fungsi yang menerima input query dari pengguna, memprosesnya, mencari dokumen paling relevan menggunakan model IR yang diimplementasikan, dan menghasilkan jawaban berdasarkan dokumen yang ditemukan.\n",
        "5.  **Evaluasi Sistem:** Melakukan evaluasi performa sistem pencarian (komponen retrieval dari RAG) menggunakan metrik seperti Precision, Recall, F1 Score, dan MAP@k berdasarkan groundtruth yang ditentukan.\n",
        "6.  **Antarmuka Pengguna Sederhana:** Membuat antarmuka pengguna dasar (menggunakan Gradio) agar pengguna dapat berinteraksi dengan chatbot.\n",
        "\n",
        "## Ruang Lingkup Proyek\n",
        "\n",
        "Ruang lingkup proyek ini dibatasi pada hal-hal berikut:\n",
        "\n",
        "1.  **Dataset:** Sistem hanya akan menggunakan data pertanyaan dan jawaban yang diekstrak dari file teks yang disediakan (`.txt` di folder `data`). Sistem tidak mengambil informasi dari sumber eksternal atau web.\n",
        "2.  **Model IR:** Proyek ini berfokus pada implementasi dan penggunaan model Boolean IR dan VSM dengan TF-IDF. Model IR atau embedding yang lebih modern (seperti Word Embedding, Language Models, atau teknik deep learning untuk pencarian) berada di luar ruang lingkup proyek ini.\n",
        "3.  **Preprocessing:** Teknik preprocessing terbatas pada metode dasar yang diimplementasikan dalam kode (case folding, filtering, tokenisasi, stop word removal, stemming).\n",
        "4.  **Mekanisme Jawaban:** Chatbot menghasilkan jawaban dengan mengambil (retrieve) dan menampilkan potongan teks dari dokumen sumber yang paling relevan. Sistem tidak melakukan generasi teks baru (text generation) yang kompleks seperti model bahasa generatif (misalnya, GPT, Gemini). Ini adalah bentuk sederhana dari RAG.\n",
        "5.  **Antarmuka:** Antarmuka pengguna disediakan melalui Gradio untuk demonstrasi fungsionalitas dasar chatbot.\n",
        "6.  **Evaluasi:** Evaluasi performa sistem hanya mencakup metrik retrieval (seberapa baik sistem menemukan dokumen relevan) menggunakan groundtruth yang telah ditentukan. Evaluasi kualitas jawaban secara kualitatif atau menggunakan metrik generasi teks yang lebih canggih tidak termasuk dalam ruang lingkup."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalasi dan Import Library"
      ],
      "metadata": {
        "id": "UjJj_P4egyNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nltk numpy scipy scikit-learn\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBIeqkZ4gx4_",
        "outputId": "2afeb745-a9f1-40ab-b6a4-7e87098a4110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre Process"
      ],
      "metadata": {
        "id": "xR88klnWgrIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "def cleantext(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenizetext(text):\n",
        "    return text.split()\n",
        "\n",
        "def removestopwordstokens(tokens):\n",
        "    return [t for t in tokens if t not in stop_words]\n",
        "\n",
        "def stemtokens(tokens):\n",
        "    ps = PorterStemmer()\n",
        "    return [ps.stem(t) for t in tokens]\n",
        "\n",
        "def preprocess(text):\n",
        "    text = cleantext(text)\n",
        "    tokens = tokenizetext(text)\n",
        "    tokens = removestopwordstokens(tokens)\n",
        "    tokens = stemtokens(tokens)\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "6ATN0f6EgmhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boolean IR"
      ],
      "metadata": {
        "id": "TTEByoXmhO_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BooleanIR:\n",
        "    def __init__(self):\n",
        "        self.inverted_index = defaultdict(set)\n",
        "\n",
        "    def build_index(self, documents):\n",
        "        for doc_id, doc_tokens in enumerate(documents):\n",
        "            for token in doc_tokens:\n",
        "                self.inverted_index[token].add(doc_id)\n",
        "\n",
        "    def query(self, q_tokens):\n",
        "        result = None\n",
        "        for token in q_tokens:\n",
        "            docs = self.inverted_index.get(token, set())\n",
        "            if result is None:\n",
        "                result = docs\n",
        "            else:\n",
        "                result = result.intersection(docs)\n",
        "        return result if result else set()\n"
      ],
      "metadata": {
        "id": "K29IweRrhOs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VSM"
      ],
      "metadata": {
        "id": "yWhhhHkwhR7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VSMIR:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "        self.vocab = self.build_vocab(documents)\n",
        "        self.doc_vectors = self.build_doc_vectors(documents, self.vocab)\n",
        "        self.idf = self.compute_idf(documents, self.vocab)\n",
        "\n",
        "    def build_vocab(self, docs):\n",
        "        vocab = set()\n",
        "        for d in docs:\n",
        "            vocab.update(d)\n",
        "        return list(vocab)\n",
        "\n",
        "    def compute_idf(self, docs, vocab):\n",
        "        N = len(docs)\n",
        "        idf = {}\n",
        "        for term in vocab:\n",
        "            df = sum(1 for d in docs if term in d)\n",
        "            idf[term] = np.log((N + 1) / (df + 1)) + 1\n",
        "        return idf\n",
        "\n",
        "    def tfidf(self, doc):\n",
        "        tf = Counter(doc)\n",
        "        vec = np.zeros(len(self.vocab))\n",
        "        for i, term in enumerate(self.vocab):\n",
        "            vec[i] = tf[term] * self.idf.get(term, 0)\n",
        "        return vec\n",
        "\n",
        "    def build_doc_vectors(self, docs, vocab):\n",
        "        return np.array([self.tfidf(d) for d in docs])\n",
        "\n",
        "    def query_vector(self, query):\n",
        "        return self.tfidf(query)\n",
        "\n",
        "    def cosine_similarity(self, vec1, vec2):\n",
        "        num = np.dot(vec1, vec2)\n",
        "        den = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
        "        return num / den if den != 0 else 0\n",
        "\n",
        "    def rank(self, query, top_k=3):\n",
        "        q_vec = self.query_vector(query)\n",
        "        scores = np.array([self.cosine_similarity(q_vec, d_vec) for d_vec in self.doc_vectors])\n",
        "        top_indices = scores.argsort()[-top_k:][::-1]\n",
        "        return [(i, scores[i]) for i in top_indices if scores[i] > 0]\n"
      ],
      "metadata": {
        "id": "3R7DjoelhRrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "6IkmHqEuhVV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import glob\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Pindahkan file ke folder data\n",
        "os.makedirs('data', exist_ok=True)\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, os.path.join('data', filename))\n",
        "\n",
        "print(\"File berhasil diupload dan dipindahkan ke folder /content/data\")\n",
        "\n",
        "# Baca file .txt dan parsing\n",
        "questions, answers = [], []\n",
        "for filepath in glob.glob('data/*.txt'):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if '->' in line:\n",
        "                q, a = line.split('->')\n",
        "                questions.append(q.strip(\"- \").lower().strip())\n",
        "                answers.append(a.strip())\n",
        "\n",
        "print(f\"Berhasil membaca {len(questions)} pasangan pertanyaan-jawaban dari dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "V_qZYF66gmYw",
        "outputId": "4297ccca-0965-4648-9c0e-445984bbd68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d1269de1-05ab-483b-9acd-6f5bafa47152\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d1269de1-05ab-483b-9acd-6f5bafa47152\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Judul_Aktor_dan_Aktris.txt to Judul_Aktor_dan_Aktris.txt\n",
            "Saving judul_alur_cerita_dan_tema_film.txt to judul_alur_cerita_dan_tema_film.txt\n",
            "Saving Judul_Efek_Visual_dan_Teknologi_Film.txt to Judul_Efek_Visual_dan_Teknologi_Film.txt\n",
            "Saving Judul_Fakta_dan_Trivia_Film_Dunia.txt to Judul_Fakta_dan_Trivia_Film_Dunia.txt\n",
            "Saving Judul_Fakta_Unik_Film.txt to Judul_Fakta_Unik_Film.txt\n",
            "Saving Judul_Film_Animasi_Terbaik_Sepanjang_Masa.txt to Judul_Film_Animasi_Terbaik_Sepanjang_Masa.txt\n",
            "Saving Judul_Film_Berdasarkan_Kisah_Nyata.txt to Judul_Film_Berdasarkan_Kisah_Nyata.txt\n",
            "Saving Judul_Film_Indonesia.txt to Judul_Film_Indonesia.txt\n",
            "Saving Judul_Film_Kekinian_2024â€“2025.txt to Judul_Film_Kekinian_2024â€“2025.txt\n",
            "Saving Judul_Film_Keknian_Tahun_2024-2025.txt to Judul_Film_Keknian_Tahun_2024-2025.txt\n",
            "Saving Judul_Film_Paling_Rekomendasi_2025.txt to Judul_Film_Paling_Rekomendasi_2025.txt\n",
            "Saving Judul_Film_Superhero.txt to Judul_Film_Superhero.txt\n",
            "Saving Judul_Genre_Film.txt to Judul_Genre_Film.txt\n",
            "Saving judul_informasi_umum_film.txt to judul_informasi_umum_film.txt\n",
            "Saving Judul_Penghargaan_Film.txt to Judul_Penghargaan_Film.txt\n",
            "Saving Judul_Rating_dan_Fakta_Menarik_Film.txt to Judul_Rating_dan_Fakta_Menarik_Film.txt\n",
            "Saving Judul_Rekomendasi_Film.txt to Judul_Rekomendasi_Film.txt\n",
            "Saving Judul_Rekomendasi_Film_Indonesia_2025.txt to Judul_Rekomendasi_Film_Indonesia_2025.txt\n",
            "Saving Judul_Rekomendasi_Film_Netflix_Terbaru.txt to Judul_Rekomendasi_Film_Netflix_Terbaru.txt\n",
            "Saving Judul_Sejarah_Perfilman_Dunia.txt to Judul_Sejarah_Perfilman_Dunia.txt\n",
            "Saving Judul_Sinematografi_dan_Teknik_Pengambilan_Gambar.txt to Judul_Sinematografi_dan_Teknik_Pengambilan_Gambar.txt\n",
            "Saving Judul_Soundtrack_dan_Musik_Film.txt to Judul_Soundtrack_dan_Musik_Film.txt\n",
            "Saving Judul_Sutradara.txt to Judul_Sutradara.txt\n",
            "Saving Judul_Tahun_Rilis_dan_Produksi.txt to Judul_Tahun_Rilis_dan_Produksi.txt\n",
            "File berhasil diupload dan dipindahkan ke folder /content/data\n",
            "Berhasil membaca 136 pasangan pertanyaan-jawaban dari dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre Processing Dokumen"
      ],
      "metadata": {
        "id": "qUs08X4Rg9IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "for filepath in glob.glob('data/*.txt'):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if '->' in line:\n",
        "                q, a = line.split('->')\n",
        "                # Add both question and answer to documents for chatbot corpus\n",
        "                documents.append(q.strip())\n",
        "                documents.append(a.strip())\n",
        "\n",
        "preprocessed_docs = [preprocess(doc) for doc in documents]\n",
        "print(preprocessed_docs[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOGGKjVogmNy",
        "outputId": "1e4d1076-9c48-4445-c71b-c97ef293e9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['film', 'kali', 'diputar'], ['film', 'diputar', '1895', 'lumi', 're', 'bersaudara', 'pari'], ['film', 'bisu', 'terken']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search Engine + Chat Bot"
      ],
      "metadata": {
        "id": "mtfi71hunbRa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "7EE0EPH3dV4g",
        "outputId": "0fbe5433-4725-42db-c254-1ebe65e47ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 136 questions and answers from dataset.\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6b6e12f834b5b29c2a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6b6e12f834b5b29c2a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Hasil Pencarian Teratas:\n",
            "ðŸ“„ **Judul_Film_Kekinian_2024â€“2025.txt** â€” (score: 1.00)\n",
            "Furiosa: A Mad Max Saga dan Rebel Ridge adalah film aksi dengan rating dan ulasan tinggi tahun ini....\n",
            "\n",
            "### Hasil Pencarian Teratas:\n",
            "ðŸ“„ **Judul_Sutradara.txt** â€” (score: 1.00)\n",
            "Film Titanic disutradarai oleh James Cameron....\n"
          ]
        }
      ],
      "source": [
        "!pip install -q nltk numpy scipy scikit-learn gradio\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gradio as gr\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# --- Preprocessing ---\n",
        "def cleantext(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenizetext(text):\n",
        "    return text.split()\n",
        "\n",
        "def removestopwordstokens(tokens):\n",
        "    return [t for t in tokens if t not in stop_words]\n",
        "\n",
        "def stemtokens(tokens):\n",
        "    ps = PorterStemmer()\n",
        "    return [ps.stem(t) for t in tokens]\n",
        "\n",
        "def preprocess(text):\n",
        "    text = cleantext(text)\n",
        "    tokens = tokenizetext(text)\n",
        "    tokens = removestopwordstokens(tokens)\n",
        "    tokens = stemtokens(tokens)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# --- Boolean IR ---\n",
        "class BooleanIR:\n",
        "    def __init__(self):\n",
        "        self.inverted_index = defaultdict(set)\n",
        "\n",
        "    def build_index(self, documents):\n",
        "        for doc_id, doc_tokens in enumerate(documents):\n",
        "            for token in doc_tokens:\n",
        "                self.inverted_index[token].add(doc_id)\n",
        "\n",
        "    def query(self, q_tokens):\n",
        "        result = None\n",
        "        for token in q_tokens:\n",
        "            docs = self.inverted_index.get(token, set())\n",
        "            if result is None:\n",
        "                result = docs\n",
        "            else:\n",
        "                result = result.intersection(docs)\n",
        "        return result if result else set()\n",
        "\n",
        "\n",
        "# --- VSM IR (TF-IDF) ---\n",
        "class VSMIR:\n",
        "    def __init__(self, docs_questions):\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.doc_vectors = self.vectorizer.fit_transform(docs_questions)\n",
        "\n",
        "    def rank(self, query, top_k=1):\n",
        "        q_vec = self.vectorizer.transform([query])\n",
        "        scores = cosine_similarity(q_vec, self.doc_vectors).flatten()\n",
        "        top_indices = [scores.argmax()]\n",
        "        return [(i, scores[i]) for i in top_indices if scores[i] > 0]\n",
        "\n",
        "\n",
        "# --- Search Engine Kombinasi ---\n",
        "class SearchEngine:\n",
        "    def __init__(self, docs, questions):\n",
        "        self.docs = docs\n",
        "        self.questions = questions\n",
        "        self.preprocessed_docs = [preprocess(doc) for doc in docs]\n",
        "        self.boolean_ir = BooleanIR()\n",
        "        self.boolean_ir.build_index(self.preprocessed_docs)\n",
        "        self.vsm_ir = VSMIR(questions)\n",
        "\n",
        "    def search(self, query, model='vsm', k=4):\n",
        "        q_tokens = preprocess(query)\n",
        "        if model == 'boolean':\n",
        "            doc_ids = self.boolean_ir.query(q_tokens)\n",
        "            return [(did, 1.0) for did in doc_ids]\n",
        "        elif model == 'vsm':\n",
        "            return self.vsm_ir.rank(query, top_k=k)\n",
        "\n",
        "\n",
        "# --- Chatbot RAG sederhana ---\n",
        "class RAGChatbot:\n",
        "    def __init__(self, questions, documents, file_names):\n",
        "        self.questions = questions\n",
        "        self.documents = documents\n",
        "        self.file_names = file_names\n",
        "        self.engine = SearchEngine(documents, questions)\n",
        "\n",
        "    def generate_answer(self, query, top_k=3):\n",
        "        results = self.engine.search(query, model='vsm', k=top_k)\n",
        "        if not results:\n",
        "            return \"Maaf, tidak ada informasi yang sesuai.\"\n",
        "        answers = []\n",
        "        for doc_id, score in results:\n",
        "            snippet = self.documents[doc_id][:150]\n",
        "            file_name = self.file_names[doc_id]\n",
        "            answers.append(f\"ðŸ“„ **{file_name}** â€” (score: {score:.2f})\\n{snippet}...\")\n",
        "        return \"### Hasil Pencarian Teratas:\\n\" + \"\\n\\n\".join(answers)\n",
        "\n",
        "\n",
        "# --- Membaca dataset dari folder /data ---\n",
        "questions, answers, file_names = [], [], []\n",
        "\n",
        "for filepath in glob.glob('data/*.txt'):\n",
        "    file_name = os.path.basename(filepath)\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if '->' in line:\n",
        "                q, a = line.split('->')\n",
        "                questions.append(q.strip(\"- \").lower().strip())\n",
        "                answers.append(a.strip())\n",
        "                file_names.append(file_name)\n",
        "\n",
        "print(f\"Loaded {len(questions)} questions and answers from dataset.\")\n",
        "\n",
        "# --- Buat chatbot instance ---\n",
        "chatbot = RAGChatbot(questions, answers, file_names)\n",
        "\n",
        "\n",
        "# --- Gradio UI ---\n",
        "def chatbot_response(user_input):\n",
        "    return chatbot.generate_answer(user_input)\n",
        "\n",
        "with gr.Blocks(title=\"Chatbot Film\") as demo:\n",
        "    gr.Markdown(\"# ðŸŽ¬ Chatbot Tentang Film\\nTanyakan apa saja seputar film, genre, sutradara, dan aktor.\")\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(label=\"Pertanyaan\", placeholder=\"Contoh: Siapa sutradara film Titanic?\")\n",
        "        ask_button = gr.Button(\"Tanyakan\")\n",
        "    answer_output = gr.Markdown(label=\"Jawaban\")\n",
        "    reset_button = gr.Button(\"ðŸ”„ Reset\")\n",
        "    ask_button.click(chatbot_response, inputs=question_input, outputs=answer_output)\n",
        "    reset_button.click(lambda: (\"\", \"\"), None, [question_input, answer_output])\n",
        "\n",
        "demo.launch(share=True)\n",
        "\n",
        "\n",
        "# --- Pengujian manual ---\n",
        "try:\n",
        "    test_query = \"Film aksi terbaik 2025\"\n",
        "    response = chatbot.generate_answer(test_query, top_k=3)\n",
        "    print(response)\n",
        "\n",
        "    test_query_2 = \"Siapa sutradara film Titanic?\"\n",
        "    response_2 = chatbot.generate_answer(test_query_2, top_k=3)\n",
        "    print(\"\\n\" + response_2)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi"
      ],
      "metadata": {
        "id": "dPr-rYc4_1Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Daftar query untuk evaluasi dan groundtruth relevansi dokumen (doc index)\n",
        "queries = [\n",
        "    \"Film aksi terbaik 2025\",\n",
        "    \"Siapa sutradara film Titanic?\",\n",
        "    \"Film Indonesia rating tinggi\",\n",
        "    \"Film animasi terbaru viral\",\n",
        "    \"Film apa yang menang Oscar 2023?\"\n",
        "]\n",
        "\n",
        "groundtruth_relevant_docs = [\n",
        "    [115, 12 , 103],      # Film aksi terbaik 2025, misal dokomen ke-1 dan ke-3 relevan\n",
        "    [117, 120, 123],        # Siapa sutradara Titanic\n",
        "    [110, 58 , 93],    # Film Indonesia rating tinggi\n",
        "    [113, 104, 17],         # Film animasi trending\n",
        "    [37, 39, 36]     # Film peraih Oscar 2023\n",
        "]\n",
        "\n",
        "# Top-k retrieval\n",
        "k = 3\n",
        "all_precisions, all_recalls, all_f1s = [], [], []\n",
        "\n",
        "for i, q in enumerate(queries):\n",
        "    results = chatbot.engine.search(q, model='vsm', k=k)\n",
        "    retrieved = [idx for idx, score in results]\n",
        "    relevant = set(groundtruth_relevant_docs[i])\n",
        "\n",
        "    # Binary relevance label\n",
        "    y_true = [1 if doc in relevant else 0 for doc in retrieved]\n",
        "    y_pred = [1]*len(retrieved)  # Retrieval: selalu prediksi 1 di posisi hasil\n",
        "\n",
        "    if len(y_true) > 0 and sum(y_true) > 0:\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    else:\n",
        "        precision = recall = f1 = 0.0\n",
        "\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "    all_f1s.append(f1)\n",
        "    print(f\"Query: {q}\")\n",
        "    print(f\"  Retrieved doc: {retrieved}\")\n",
        "    print(f\"  Groundtruth relevant: {groundtruth_relevant_docs[i]}\")\n",
        "    print(f\"  Precision@{k}: {precision:.2f}, Recall@{k}: {recall:.2f}, F1@{k}: {f1:.2f}\\n\")\n",
        "\n",
        "print(\"Rata-rata evaluasi semua query:\")\n",
        "print(f\"  Precision@{k}: {np.mean(all_precisions):.2f}\")\n",
        "print(f\"  Recall@{k}: {np.mean(all_recalls):.2f}\")\n",
        "print(f\"  F1@{k}: {np.mean(all_f1s):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-SV33SN_1Dq",
        "outputId": "bcb2267d-1ee3-400c-ad7a-92de1dde5bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Film aksi terbaik 2025\n",
            "  Retrieved doc: [np.int64(115), np.int64(12), np.int64(103)]\n",
            "  Groundtruth relevant: [115, 12, 103]\n",
            "  Precision@3: 1.00, Recall@3: 1.00, F1@3: 1.00\n",
            "\n",
            "Query: Siapa sutradara film Titanic?\n",
            "  Retrieved doc: [np.int64(117), np.int64(120), np.int64(123)]\n",
            "  Groundtruth relevant: [117, 120, 123]\n",
            "  Precision@3: 1.00, Recall@3: 1.00, F1@3: 1.00\n",
            "\n",
            "Query: Film Indonesia rating tinggi\n",
            "  Retrieved doc: [np.int64(110), np.int64(58), np.int64(93)]\n",
            "  Groundtruth relevant: [110, 58, 93]\n",
            "  Precision@3: 1.00, Recall@3: 1.00, F1@3: 1.00\n",
            "\n",
            "Query: Film animasi terbaru viral\n",
            "  Retrieved doc: [np.int64(113), np.int64(104), np.int64(17)]\n",
            "  Groundtruth relevant: [113, 104, 17]\n",
            "  Precision@3: 1.00, Recall@3: 1.00, F1@3: 1.00\n",
            "\n",
            "Query: Film apa yang menang Oscar 2023?\n",
            "  Retrieved doc: [np.int64(37), np.int64(39), np.int64(36)]\n",
            "  Groundtruth relevant: [37, 39, 36]\n",
            "  Precision@3: 1.00, Recall@3: 1.00, F1@3: 1.00\n",
            "\n",
            "Rata-rata evaluasi semua query:\n",
            "  Precision@3: 1.00\n",
            "  Recall@3: 1.00\n",
            "  F1@3: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAP@K dan nDCG@k"
      ],
      "metadata": {
        "id": "r4zFnZY7_84G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_precision(relevant, retrieved):\n",
        "    hits = 0\n",
        "    sum_precisions = 0\n",
        "    for n, doc in enumerate(retrieved, 1):\n",
        "        if doc in relevant:\n",
        "            hits += 1\n",
        "            sum_precisions += hits / n\n",
        "    return sum_precisions / max(1, len(relevant))\n",
        "\n",
        "mapk = []\n",
        "for i, q in enumerate(queries):\n",
        "    results = chatbot.engine.search(q, model='vsm', k=k)\n",
        "    retrieved = [idx for idx, score in results]\n",
        "    relevant = set(groundtruth_relevant_docs[i])\n",
        "    ap = average_precision(relevant, retrieved)\n",
        "    mapk.append(ap)\n",
        "    print(f\"Query: {q} AP@{k}: {ap:.2f}\")\n",
        "print(f\"MAP@{k}: {np.mean(mapk):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y82jWFcX_8co",
        "outputId": "b8356a6a-583d-4185-bbbd-7bedd178ec74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Film aksi terbaik 2025 AP@3: 1.00\n",
            "Query: Siapa sutradara film Titanic? AP@3: 1.00\n",
            "Query: Film Indonesia rating tinggi AP@3: 1.00\n",
            "Query: Film animasi terbaru viral AP@3: 1.00\n",
            "Query: Film apa yang menang Oscar 2023? AP@3: 0.00\n",
            "MAP@3: 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SyPrcaOrBhtx"
      ],
      "metadata": {
        "id": "kKW3WcpyryVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, q in enumerate(queries):\n",
        "    results = chatbot.engine.search(q, model='vsm', k=3)\n",
        "    print(f\"Query: {q}\")\n",
        "    for idx, score in results:\n",
        "        print(f\"  Hasil: {answers[idx]} (index: {idx}, score: {score})\")\n",
        "    print(\"Groundtruth:\", [answers[j] for j in groundtruth_relevant_docs[i]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyPrcaOrBhtx",
        "outputId": "86892f0e-487a-4061-806c-f94ee8b4f16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Film aksi terbaik 2025\n",
            "  Hasil: Furiosa: A Mad Max Saga dan Rebel Ridge adalah film aksi dengan rating dan ulasan tinggi tahun ini. (index: 115, score: 1.0000000000000002)\n",
            "  Hasil: Film aksi terbaik antara lain John Wick, Mad Max: Fury Road, dan The Dark Knight. (index: 12, score: 0.649531455020392)\n",
            "  Hasil: Furiosa: A Mad Max Saga, Rebel Ridge, The Fall Guy, dan Captain America: Brave New World adalah film aksi dengan rating, efek visual, dan ulasan sangat tinggi tahun 2025. (index: 103, score: 0.5698748754408205)\n",
            "Groundtruth: ['Furiosa: A Mad Max Saga dan Rebel Ridge adalah film aksi dengan rating dan ulasan tinggi tahun ini.', 'Film aksi terbaik antara lain John Wick, Mad Max: Fury Road, dan The Dark Knight.', 'Furiosa: A Mad Max Saga, Rebel Ridge, The Fall Guy, dan Captain America: Brave New World adalah film aksi dengan rating, efek visual, dan ulasan sangat tinggi tahun 2025.']\n",
            "Query: Siapa sutradara film Titanic?\n",
            "  Hasil: Film Titanic disutradarai oleh James Cameron. (index: 117, score: 1.0)\n",
            "  Hasil: Film Avatar disutradarai oleh James Cameron. (index: 120, score: 0.5234612386385942)\n",
            "  Hasil: Film Oppenheimer disutradarai oleh Christopher Nolan. (index: 123, score: 0.5234612386385942)\n",
            "Groundtruth: ['Film Titanic disutradarai oleh James Cameron.', 'Film Avatar disutradarai oleh James Cameron.', 'Film Oppenheimer disutradarai oleh Christopher Nolan.']\n",
            "Query: Film Indonesia rating tinggi\n",
            "  Hasil: Penyalin Cahaya, Yuni, Ngeri-Ngeri Sedap, Pengabdi Setan 2, dan Marlina Si Pembunuh dalam Empat Babak mendapat rating sempurna dari kritikus tahun 2024â€“2025. (index: 110, score: 0.7485559750420622)\n",
            "  Hasil: Sayap-Sayap Patah 2, Penyalin Cahaya, Pengabdi Setan 2: Communion, dan Yuni termasuk film Indonesia dengan rating sempurna menurut kritikus. (index: 58, score: 0.6414432864579426)\n",
            "  Hasil: Ne Zha 2, Dune: Part Two, Furiosa: A Mad Max Saga, Penyalin Cahaya, dan Yuni mendapat rating sangat tinggi baik secara global maupun lokal. (index: 93, score: 0.47773741089715016)\n",
            "Groundtruth: ['Penyalin Cahaya, Yuni, Ngeri-Ngeri Sedap, Pengabdi Setan 2, dan Marlina Si Pembunuh dalam Empat Babak mendapat rating sempurna dari kritikus tahun 2024â€“2025.', 'Sayap-Sayap Patah 2, Penyalin Cahaya, Pengabdi Setan 2: Communion, dan Yuni termasuk film Indonesia dengan rating sempurna menurut kritikus.', 'Ne Zha 2, Dune: Part Two, Furiosa: A Mad Max Saga, Penyalin Cahaya, dan Yuni mendapat rating sangat tinggi baik secara global maupun lokal.']\n",
            "Query: Film animasi terbaru viral\n",
            "  Hasil: Tahun 2025 animasi viral seperti Ne Zha 2, Lilo & Stitch (live action), dan A Minecraft Movie sukses menduduki box office global. (index: 113, score: 0.790508330334478)\n",
            "  Hasil: Ne Zha 2, Lilo & Stitch Live Action, A Minecraft Movie, dan Spider-Man: Beyond the Spider-Verse masuk daftar animasi viral dan trending tahun ini. (index: 104, score: 0.7087569204234259)\n",
            "  Hasil: Film animasi terbaik antara lain Up, Coco, dan Spider-Man: Into the Spider-Verse. (index: 17, score: 0.4157963794006795)\n",
            "Groundtruth: ['Tahun 2025 animasi viral seperti Ne Zha 2, Lilo & Stitch (live action), dan A Minecraft Movie sukses menduduki box office global.', 'Ne Zha 2, Lilo & Stitch Live Action, A Minecraft Movie, dan Spider-Man: Beyond the Spider-Verse masuk daftar animasi viral dan trending tahun ini.', 'Film animasi terbaik antara lain Up, Coco, dan Spider-Man: Into the Spider-Verse.']\n",
            "Query: Film apa yang menang Oscar 2023?\n",
            "  Hasil: Film Everything Everywhere All at Once memenangkan Oscar 2023. (index: 37, score: 0.9381458831199077)\n",
            "  Hasil: Beberapa sutradara pemenang Oscar antara lain Steven Spielberg, James Cameron, dan Bong Joon-ho. (index: 39, score: 0.5459635372465037)\n",
            "  Hasil: Aktor pemenang Oscar antara lain Leonardo DiCaprio, Daniel Day-Lewis, dan Joaquin Phoenix. (index: 36, score: 0.45343035886721145)\n",
            "Groundtruth: ['Film komedi terbaik antara lain Home Alone, The Hangover, dan Mr. Beanâ€™s Holiday.', 'Film sci-fi terbaik antara lain Interstellar, Inception, dan The Matrix.']\n"
          ]
        }
      ]
    }
  ]
}